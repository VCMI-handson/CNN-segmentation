{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hands-on 2020",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VCMI-handson/CNN-segmentation/blob/master/Hands_on_2020_CNN-segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIVm3Qd8Ms4q",
        "colab_type": "text"
      },
      "source": [
        "# Hands-on Session 3: CNN Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay90gWDMyRAj",
        "colab_type": "text"
      },
      "source": [
        "##**목차**\n",
        "\n",
        "###*   Step 0. 실습환경 셋팅\n",
        "###*   Step 1. 데이터셋 불러들이기\n",
        "###*   Step 2. 데이터 살펴보기\n",
        "###*   Step 3. 첫번째 영상분할 모델 (FCN32s)\n",
        "###*   Step 4. 두번째 영상분할 모델 (FCN8s)\n",
        "###*   Step 5. 두번째 영상분할 모델 개선 (FCN2s)\n",
        "###*   Step 6. 두번째 영상분할 모델 개선 (FCN8s with Deconvolution)\n",
        "###*   Step 7. 세번째 영상분할 모델 (Unet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7rXugkuL1Fe",
        "colab_type": "text"
      },
      "source": [
        "# Step 0. 실습환경 셋팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bpcPhkPvdV3",
        "colab_type": "text"
      },
      "source": [
        "##1. Google 계정 로그인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LovHsPLMoa6",
        "colab_type": "text"
      },
      "source": [
        "##2. 하단 링크 접속\n",
        "\n",
        "##**[https://colab.research.google.com/github/VCMI-handson/CNN-segmentation](https://colab.research.google.com/github/VCMI-handson/CNN-segmentation)**\n",
        "\n",
        "\n",
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1P4jZoyPdzPdvBg3fDKUekByyi3iq9z9G)\n",
        "\n",
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1puX_WuNVBWdsm9qJwulz2uTniUcDYaoE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzYzJG6a30Xx",
        "colab_type": "text"
      },
      "source": [
        "# Step 1. 데이터셋 불러들이기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIVjUrRO4D1y",
        "colab_type": "text"
      },
      "source": [
        "##Lung segmentation from Chest X-Ray dataset\n",
        "\n",
        "https://www.kaggle.com/nikhilpandey360/lung-segmentation-from-chest-x-ray-dataset\n",
        "\n",
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1Oa-iXD9IMQre4Ao7CyslObAtZRBCskvB)\n",
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1lz6EHChnPtCbkH6XI_Tm0_tEpJWnkb1a)\n",
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1ZK09wg_gJGGCyKnvkSkgxcpl1Pz4esDa)\n",
        "\n",
        "\n",
        "###Lung_Segmentation.zip\n",
        "###256x256x3\n",
        "###566 datasets [image, label]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Msw9-ZMFhQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf *\n",
        "!wget https://github.com/mi2rl/datasets/raw/master/Lung_Segmentation.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuS9w6XcFtw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U547OnoGuh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip Lung_Segmentation.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndHDpKrvGyzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "data_path = \"./Lung_Segmentation\"\n",
        "\n",
        "files = os.listdir(os.path.join(data_path, 'image'))\n",
        "file_headers = []  #python list\n",
        "for f in files:\n",
        "    f1 = os.path.splitext(f)[0]\n",
        "    file_headers.append(f1)\n",
        "#print(file_headers)\n",
        "\n",
        "X_all = np.zeros((len(file_headers), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "y_all = np.zeros((len(file_headers), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
        "\n",
        "count = 0\n",
        "for fh in file_headers:\n",
        "    f1 = os.path.join(data_path, 'image', '{}.png'.format(fh))\n",
        "    l1 = os.path.join(data_path, 'label', '{}.png'.format(fh))\n",
        "    \n",
        "    img = imread(f1)[:,:,:IMG_CHANNELS]\n",
        "    mask = imread(l1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "    X_all[count] = img\n",
        "    y_all[count] = mask\n",
        "    \n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5PJph8W3h39",
        "colab_type": "text"
      },
      "source": [
        "**딥러닝을 위한 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edXzK3BiIKoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all = X_all.astype('float32') / 255.\n",
        "y_all = y_all.astype('float32') / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgZM4cCm3kT0",
        "colab_type": "text"
      },
      "source": [
        "**학습, 검증, 테스트 데이터 셋으로 분리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAcUzVN3IP1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNyzOYeNISvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('X_train',X_train.shape)\n",
        "print('X_valid',X_valid.shape)\n",
        "print('X_test',X_test.shape)\n",
        "print('y_train',y_train.shape)\n",
        "print('y_valid',y_valid.shape)\n",
        "print('y_test',y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj-8QGAM4uLF",
        "colab_type": "text"
      },
      "source": [
        "# Step 2. 데이터 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA3G2uHrIXeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotTrainData(a,b,c):\n",
        "    for i in range(3):\n",
        "        ix = np.random.randint(0, len(a))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"X_\" + c)\n",
        "        plt.imshow(a[ix])\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"y_\" + c)\n",
        "        plt.imshow(np.squeeze(b[ix]), 'gray')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "plotTrainData(X_train,y_train, 'train')\n",
        "plotTrainData(X_valid,y_valid, 'valid')\n",
        "plotTrainData(X_test,y_test, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0QeCScgWYAH",
        "colab_type": "text"
      },
      "source": [
        "# Step 3. 첫번째 영상분할 모델 (FCN32s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckBjUdszUVcQ",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=13hnpVhEYBHXytLoBQ7R19zvaHBGLEiou)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL9L0H9DIrhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Activation, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import *\n",
        "\n",
        "def fcn32s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #VGG16\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "        \n",
        "    #FCN32s\n",
        "    conv_t1 = UpSampling2D(size = (32,32))(pool_5)    \n",
        "    conv_t2 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv_t1)   \n",
        "    pred = Activation('sigmoid')(conv_t2)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61FPu14nYGLf",
        "colab_type": "text"
      },
      "source": [
        "**loss 계산**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSCqquL5Ivhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK_KZldyYKZt",
        "colab_type": "text"
      },
      "source": [
        "**생성한 모델 수행**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apvc5n3XIyGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model = fcn32s()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-32s.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([-1.0, -0.0])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArSr-PgKiwz1",
        "colab_type": "text"
      },
      "source": [
        "**결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfhEZaRjbU4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotPredictions(X_train_, y_train_, X_valid_, y_valid_, X_test_, y_test_, simpleFCN):\n",
        "    model = simpleFCN     \n",
        "\n",
        "    ix = np.random.randint(0, len(X_train_))\n",
        "    input_ = X_train_[ix:ix+1]\n",
        "    mask_ = y_train_[ix:ix+1]\n",
        "    preds_train = model.predict(input_)\n",
        "    preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_train\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_train\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_train_t[0][:,:,0], 'gray')\n",
        "    plt.show()\n",
        "    \n",
        "    ix = np.random.randint(0, len(X_valid_))\n",
        "    input_ = X_valid_[ix:ix+1]\n",
        "    mask_ = y_valid_[ix:ix+1]\n",
        "    preds_valid = model.predict(input_)\n",
        "    preds_valid_t = (preds_valid > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_valid\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_valid\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_valid_t[0][:,:,0], 'gray')\n",
        "    plt.show()\n",
        "    \n",
        "    ix = np.random.randint(0, len(X_test_))\n",
        "    input_ = X_test_[ix:ix+1]\n",
        "    mask_ = y_test_[ix:ix+1]\n",
        "    preds_test = model.predict(input_)\n",
        "    preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_test\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_test\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_test_t[0][:,:,0], 'gray')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9rnmblFastj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i4w6Gvxhn3Y",
        "colab_type": "text"
      },
      "source": [
        "# Step 4.  두번째 영상분할 모델 (FCN8s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r86bSvSpi3hs",
        "colab_type": "text"
      },
      "source": [
        "**Skip Connection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5UF44qZoS5p",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1m7zNPxTiDMqYEmcNm35020vTUhJXkWRY)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbVyPkCYa8GA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Add, Activation, BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "def fcn8s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #fcn32s\n",
        "    #conv_t1 = UpSampling2D(size = (32,32))(pool_5)    \n",
        "    #conv_t2 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv_t1)\n",
        "    #pred = Activation('sigmoid')(conv_t2)\n",
        "    \n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        "    \n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    conv_t1 = UpSampling2D(size = (2,2))(conv8)     \n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    conv_t2 = UpSampling2D(size = (2,2))(fuse_1)\n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "   \n",
        "    conv_t3 = UpSampling2D(size = (8,8))(fuse_2)\n",
        "    \n",
        "    pred = Activation('sigmoid')(conv_t3)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNy43JMviHVT",
        "colab_type": "text"
      },
      "source": [
        "**생성한 모델 수행**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEfkORdnbApL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model = fcn8s()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-8s.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([-1.0, 0.0])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ0Yq79Xi56K",
        "colab_type": "text"
      },
      "source": [
        "**결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm03mqRwn8iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b3eFAewjHHu",
        "colab_type": "text"
      },
      "source": [
        "# Step 5. 두번째 영상분할 모델 개선 (FCN2s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_DQ_rIjYqz",
        "colab_type": "text"
      },
      "source": [
        "**Skip Connection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLBxVfwi9US",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Add,  Activation, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import *\n",
        "\n",
        "def fcn2s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        "\n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    conv_t1 = UpSampling2D(size = (2,2))(conv8)    \n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    conv_t2 = UpSampling2D(size = (2,2))(fuse_1)\n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "  \n",
        "    #(64, 64)\n",
        "    score_pool2 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_2)\n",
        "    conv_t3 = UpSampling2D(size = (2,2))(fuse_2)\n",
        "    fuse_3 = Add()([conv_t3,score_pool2])\n",
        "\n",
        "    #(128, 128)\n",
        "    score_pool1 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_1)\n",
        "    conv_t4 = UpSampling2D(size = (2,2))(fuse_3)\n",
        "    fuse_4 = Add()([conv_t4,score_pool1])\n",
        "      \n",
        "    conv_t5 = UpSampling2D(size = (2,2))(fuse_4)\n",
        "    \n",
        "    pred = Activation('sigmoid')(conv_t5)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMhFmkp7jysR",
        "colab_type": "text"
      },
      "source": [
        "**생성한 모델 수행**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYv1-FdCjfmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model = fcn2s()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-2s.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([-1.0, -0.0])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxhhuA7jd1Y",
        "colab_type": "text"
      },
      "source": [
        "**결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-cBl5NCj0l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFQI-lXdj3k9",
        "colab_type": "text"
      },
      "source": [
        "# Step 6. 두번째 영상분할 모델 개선 (FCN8s with Deconvolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFMqSgAikC3i",
        "colab_type": "text"
      },
      "source": [
        "**Deconvolution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ed4VHfgpHWr",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1lo8xBRyU_FWrlsNsRMklaPa8h6nc7DMJ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp8wPjChkEe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Add, Conv2DTranspose, BatchNormalization, Activation\n",
        "from keras.models import Model\n",
        "from keras.optimizers import *\n",
        "\n",
        "def fcn8s_deconv():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "    \n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        " \n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    \n",
        "    conv_t1 = Conv2DTranspose(1, kernel_size=(2,2), strides=(2,2), padding=\"same\")(conv8)\n",
        "    conv_t1 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t1)\n",
        "    conv_t1 = BatchNormalization()(conv_t1)\n",
        "    conv_t1 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t1)\n",
        "    conv_t1 = BatchNormalization()(conv_t1)\n",
        "    \n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    \n",
        "    conv_t2 = Conv2DTranspose(1, kernel_size=(2,2), strides=(2,2),padding=\"same\")(fuse_1)\n",
        "    conv_t2 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t2)\n",
        "    conv_t2 = BatchNormalization()(conv_t2)\n",
        "    conv_t2 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t2)\n",
        "    conv_t2 = BatchNormalization()(conv_t2)\n",
        "    \n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "\n",
        "    #(32, 32) x 8 = 256\n",
        "    conv_t3 = Conv2DTranspose(1, kernel_size=(8,8), strides=(8,8), padding=\"same\")(fuse_2)\n",
        "    conv_t3 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t3)\n",
        "    conv_t3 = BatchNormalization()(conv_t3)\n",
        "    conv_t3 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t3)\n",
        "    conv_t3 = BatchNormalization()(conv_t3)\n",
        "    \n",
        "    pred = Activation('sigmoid')(conv_t3)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKky4KOulIT_",
        "colab_type": "text"
      },
      "source": [
        "**생성한 모델 수행**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GxKfcVElJTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model = fcn8s_deconv()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-8s_deconv.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([-1.0, -0.0])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnmXQKb2lJpa",
        "colab_type": "text"
      },
      "source": [
        "**결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYUg8sSflXt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoznsNXalnTW",
        "colab_type": "text"
      },
      "source": [
        "# Step 7. 세번째 영상분할 모델 (Unet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfjEcQB9p43M",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1vSx_i5kIBjaHTk5pvaa-yOXTX_69UzZh)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNLSqxwzly3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras import backend as keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "def unet(input_size=(256,256,3)):\n",
        "    inputs = Input(input_size)\n",
        "    \n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    \n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    \n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    \n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    \n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "    return Model(inputs=[inputs], outputs=[conv10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljf4PB_al3JX",
        "colab_type": "text"
      },
      "source": [
        "**생성한 모델 수행**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKk57f5fl244",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model = unet()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('unet.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([-1.0, -0.0])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pTUUy_Zl8l-",
        "colab_type": "text"
      },
      "source": [
        "**결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv_PRoIjl-xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}